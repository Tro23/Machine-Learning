{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "seventh-ceiling",
   "metadata": {},
   "source": [
    "$$\n",
    "\\newcommand{\\eby}[1]{e^{{n,#1}_{n,#1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-tennessee",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-fiction",
   "metadata": {},
   "source": [
    "### Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-frederick",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> Brief Overview of Time Series</li>\n",
    "    <li> State Space models for Time Series</li>\n",
    "    <li> Machine Learning methods for Time Series</li>\n",
    "    <li> Deep Learning for Time Series</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-haiti",
   "metadata": {},
   "source": [
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-joint",
   "metadata": {},
   "source": [
    "### What are Time Series?\n",
    "\n",
    "Anything to with time. Period. \n",
    "Temporal- Sets of time over a sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-evaluation",
   "metadata": {},
   "source": [
    "### Tasks for Time Series Analysis\n",
    "\n",
    "<ul>\n",
    "    <li>Visualisation and Exploratory data analysis:<ul>\n",
    "        <li> Understanting <b>Temporal Behavior</b> of data : <b> Seasonality</b>, <b> Stationarity</b></li>\n",
    "        <li> Indentify underlying distributions and nature of Temporal process producing data (<b> Hypothesis Testing </b>) </li></ul></li>\n",
    "    <li> Estimation of past, present and future values: <b> Filtering vs Forecasting</b></li>\n",
    "    <li> <b>Classification</b> of Time Series</li>\n",
    "    <li> <b>Anomaly Detection</b> of Outlier Points withing Time Series</li>\n",
    "    </ul>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-investing",
   "metadata": {},
   "source": [
    "### Time Series vs Cross-Sectional Data (data set at a 'fixed point' in time)\n",
    "\n",
    "<ul>\n",
    "    <li>More Opportunities for <b>Missing</b> data points<ul><li>Quite Challenging following the same samples through time. <i>Ex: Colorado river data, there were discontinuation of data because of funding difference from the change in government</i></li></ul></li>\n",
    "    <li><b>Often</b> there is a high degree of correlation between data points<ul><li>Values in the Past predict values in the Future.</li><li>This is good for <i>prediction</i>but bad for models that assume <i>independent inputs</i></li></ul></li>\n",
    "    <li><b>Time Stamps</b> or other measures of distance travelled along the <b>Temporal Axis</b> introduce all kinds of data messiness<ul><li>Time Zones, Frequency irregularities, etc.</li></ul></li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-vietnam",
   "metadata": {},
   "source": [
    "### Characteristics of Time Series Data:\n",
    "<ul>\n",
    "    <li>Data is Collected Sequentially: one axis is monotonically increasing</li>\n",
    "    <li>Structure is characterized across data points:<ul><li>Sesonality & Cycles</li><li>Autocorrelation<ul><li>It is the correlation between $ t_i \\; and \\; t_{i-1}$; in a probabilistic sense.</li><li>Can't always eye-ball over the time series data, realizing the autocorrelation helps us understand the correlation of data over time.</li></ul></li><li>Trends</li></ul></li>\n",
    "    <li>Stochastic behavior even as to behavioral regime<ul><li>Change point/ regime shifts vs drift/ gradual change</li></ul></li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-basics",
   "metadata": {},
   "source": [
    "### Special Concers for Time Series data\n",
    "    Running special Diagnostics, that are time aware .\n",
    "<ul>\n",
    "    <li>Correlated Errors</li>\n",
    "    <li>Cross-Validation<ul><li>Information from future might leak backwards!</li></ul></li>\n",
    "    <li>Lookahead (Forecasting)<ul><li>The data might just be a time stamp, or of past and no relation to future, etc.</li></ul></li>\n",
    "    <li>Stationarity</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plain-tongue",
   "metadata": {},
   "source": [
    "### State Space Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-sullivan",
   "metadata": {},
   "source": [
    "#### Box-Jenkins ARIMA modeling\n",
    "<ul>\n",
    "    <li></li>\n",
    "    <li></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-salad",
   "metadata": {},
   "source": [
    "### Kalman Filter\n",
    "\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Kalman Filter.png\" width=\"800\">\n",
    "\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Kalman Filter_2.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-rebate",
   "metadata": {},
   "source": [
    "### Evaluating Models of Time Series\n",
    "\n",
    "#### Akaike Information Criterion( AIC)\n",
    "\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\AIC.png\" width=\"400\">\n",
    "\n",
    "##### 'K' is number of Parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-order",
   "metadata": {},
   "source": [
    "### Models that were fit in lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-arctic",
   "metadata": {},
   "source": [
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Models_Fit.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-holder",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "##### R does a better job at unified interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-lottery",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMMs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-minutes",
   "metadata": {},
   "source": [
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Hidden_Markov.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-flight",
   "metadata": {},
   "source": [
    "#### Why is HMMs a State Space?\n",
    "    It's the notion that there's a sequence of countable states that aren't observable but their output is observable. The generator is hidden but the outcome affects the process, which are observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-knight",
   "metadata": {},
   "source": [
    "Most importantly, Hmms capture the underline dynamics in probabilistic sense. ARIMA models do not capture the Non-Linear Dynamics. It's all about regime switching, where we might realize or know the probability of the regime switching.\n",
    "<br>\n",
    "</br>\n",
    "Hmms helps in the sense that we can have a probabilistic understanding of possible regime state change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-prisoner",
   "metadata": {},
   "source": [
    "### HMMs\n",
    "<ul>\n",
    "    <li><b>State Space Models:</b> Observations are an indicator of underlying state</li>\n",
    "    <li><b>Markov Process:</b> Present is the complete manifestation of past, if present is known it is a sufficient statistic.</li>\n",
    "    <li><b>Parameter Estimation:</b> Baum-Wlech Algorithm, special case of <i>EM Algorithm</i></li>\n",
    "    <li><b>Smoothing/State Labeling:</b> Viterbi Algorithm, which finds the most likely sequence of states that results in the observed sequence of states.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-burning",
   "metadata": {},
   "source": [
    "### Baum- Welch Algorithm. (EM Iterations)\n",
    "    Brief Over-view of The Algorithm\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Baum-Welch.png\" width=\"600\">\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Baum-Welch-2.png\" width=\"600\">\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Baum-Welch-3.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-yesterday",
   "metadata": {},
   "source": [
    "A is State Transition Matrix, B is How likely is particular y is assuming a particular underlying state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-stationery",
   "metadata": {},
   "source": [
    "#### Dynamic Programming\n",
    "\n",
    "Limited Countable Ordered Set of Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-broadway",
   "metadata": {},
   "source": [
    "### Machine Learning For Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accredited-treasurer",
   "metadata": {},
   "source": [
    "Can not just feed in the series so instead, we send in the <b> Features that are generated from the time series </b>\n",
    "<br></br>\n",
    "In State SPace, model is one long time series( i/p). In ML the model doesn't care about temporal subset of time series, instead we take time windows and chop up the time series which becomes the sample (i/p)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-crack",
   "metadata": {},
   "source": [
    "### Time Series Feature Generation\n",
    "    Sure is Computationally Taxing\n",
    "<ul>\n",
    "    <li> This Happens because the time series might not be sufficiently described by a set of limited features </li>\n",
    "    <li>Feature Importance analysis is a must.</li></ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-colleague",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping\n",
    "    Distance Metric for Time Series.\n",
    "    Mapping of time series to one another. (cool!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-fiber",
   "metadata": {},
   "source": [
    "## Deep Learning for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-brunswick",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network (RNNs)\n",
    "#### - LSTM (Long Short Term Memory Networks)\n",
    "     It's how we put a time series into a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-receptor",
   "metadata": {},
   "source": [
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\Recurrent Neural Network.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-brown",
   "metadata": {},
   "source": [
    "LSTM is a special kind of RNN. It's an amalgamation of <i>Cells</i>. Each cell has features to memories the series of data in modules as it passes into and through it.\n",
    "<br>\n",
    "</br>\n",
    "The cells have the ability to pass the modules of past to the present cell to determine the yield required. Each module of past is a series of data that contain information to be processed.<b>The cells are made of sigmoid activation function</b>, so that we can output or modify or filter based on threshold.\n",
    "<br>\n",
    "</br>\n",
    "\n",
    "<b>Cells are of three types, called <i>Gates</i>:</b>\n",
    "<ul>\n",
    "    <li><b>Forget Gate:</b> This gate outputs 'a complete passage of data' or 'a complete blockage of data'; where passage is '<b>1</b>' and blockage is '<b>0</b>' respectively.</li>\n",
    "    <li><b>Memory Gate:</b> This Gate passes through series of cells, sigmoid and tanh, respectively. Where The Sigmoid function chooses the seriesed data to be filtered, based on threshold and then, The TanH function filters/ modifies the output.</li>\n",
    "    <li><b>Output Gate:</b> This Gate yields the output based on a threshold between 0 or 1</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-millennium",
   "metadata": {},
   "source": [
    "### Step-by-Step Walk Through of LSTM\n",
    "<ul>\n",
    "    <li></li>\n",
    "    <li></li>\n",
    "    <li></li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-bibliography",
   "metadata": {},
   "source": [
    "#### GRU - Gated Recurrent Unit\n",
    "<img src=\"C:\\\\Users\\\\Rohit\\\\Deep Learning\\\\GRU.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-lease",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks.\n",
    "    Is also sensitive/responsive to time series. They are temporal aware or can be made to. \n",
    "    \n",
    "    Causal CNN???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-burton",
   "metadata": {},
   "source": [
    "## LSTNet \n",
    " - Combination of CNN ( multivariate and seasonality) and Recurrent Neural Network( reflects seasonality or memory of time series) and then, a AutoRegressive Component. \n",
    " - Which implies a statistical integration into the deep learning, which would be helpful for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-clause",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
